\documentclass[]{scrartcl}

%opening
\title{High Availability and Scalability: ELB and ASG}
\author{Uday Manchanda}

\begin{document}

\maketitle

\section{High Availability and Scalability}
\begin{itemize}
	\item Scalability: application/system can handle greater loads by adapting
	\item Two types:
	\begin{itemize}
		\item Vertical
		\item Horizontal (aka elasticity)
	\end{itemize}
	\item Linked to, but from different from, High Availability
\end{itemize}

\subsection{Vertical Scalability}
\begin{itemize}
	\item increasing the size of the instances
	\item EX: Application runs on t2.micro, scaling vertically means running on t2.large
	\item Very common for non distributed systems like DBs
	\item hardware limits
\end{itemize}

\subsection{Horizontal Scalability}
\begin{itemize}
	\item increasing the number of instances/systems
	\item Implies distributed systems
	\item Cloud makes it easier to scale horizontally
	\item Scale out/in
	\item ASGs/LBs
\end{itemize}

\subsection{High Availability}
\begin{itemize}
	\item Goes hand in hand with horizontal scaling
	\item Running your application in multiple AZs
\end{itemize}

\section{Elastic Load Balancing}
\begin{itemize}
	\item Load Balancers: servers that forward internet traffic to multiple servers downstream
	\item Users interface with the load balancer which serve traffic to the EC2 instances
	\item Why use it?
	\begin{itemize}
		\item Spread load
		\item Expose single point of access (DNS) to application
		\item Seamlessly handle failures
		\item Health checks, SSL termination
		\item Separate public from private traffic
	\end{itemize}
	\item ELB: managed load balancer from AWS
	\begin{itemize}
		\item Classic Load Balancer v1 (HTTP/HTTPS/TCP)
		\item Application Load Balancer v2 (HTTP/HTTPS/WebSocket)
		\item Network Load Balancer v2 (TCP/TLS/UDP)
	\end{itemize}
	\item Can scale but not instantaneously
	\item ELB access logs will access requests
	\item CW metrics gives aggregate metrics
	\item NLBs exposed a public static IP whereas an ALB/CLB exposes a static DNS
\end{itemize}

\subsection{Health Checks}
\begin{itemize}
	\item enable the LB to know if instances it forwards traffic to are available to reply to requests
	\item Done on a port and route (usually $/health$). If not 200, traffic is unhealthy
\end{itemize}

\subsection{Security Groups}
\begin{itemize}
	\item users $\rightarrow$ (HTTP/HTTPS) LB $\rightarrow$ (HTTP restricted to LB) EC2
\end{itemize}

\subsection{Sticky Sessions}
\begin{itemize}
	\item aka session afinity
	\item it is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer
	\item Works for CLB and ALBs
	\item The "cookie" is used for stickiness has an expiration date
	\item Use case: user doesn't lose his session data
	\item Cookie Names
	\begin{itemize}
		\item Application based cookies
		\begin{itemize}
			\item custom cookie - generated by the target, can include any custom attributes, must be specified for each TG
			\item Application cookie - generated by the LB, named AWSALBAPP
		\end{itemize}
		\item Duration based cookies: generated by LB, always either AWSALB or AWSELB
	\end{itemize} 
\end{itemize}

\subsection{Cross Zone Load Balancing}
\begin{itemize}
	\item Assume you have two load balancers in two different AZs with multiple EC2 instances
	\item Client is accessing these LBs
	\item with cross zone load balancing each LB instance distributes evenly across all registered instances in all AZs
	\item So the client traffic gets routed evenly to each AZ
	\item If AZ1 has 2 EC2 instances and AZ2 has 8 EC2 instances, each EC2 instance gets 10 percent of the traffic (total of 10 instances)
	\item without cross zone load balancing, the traffic is still routed evenly between each AZ
	\item But then each EC2 in AZ1 gets 25 percent of the traffic and AZ2 gets 6.25 percent
	\item Always on for ALBs but disabled by default in NLBs
\end{itemize}

\subsection{SSL certificates}
\begin{itemize}
	\item SSL cert allows traffic between your clients and your LB to be encrypted in transit (in flight encryption)
	\item SSL = secure sockets layer
	\item TLS = transport layer security, newer version
	\item They have an expiration date
	\item HTTPS = HTTP + SSL
	\item LB does something called SSL certificate termination which allows it to talk to EC2 instances via HTTP
	\item ACM = AWS certificate manager
	\item Clients can use SNI (server name indication) to specify the hostname they reach
	\item SNI
	\begin{itemize}
		\item solves the problem of loading multiple SSL certificates onto one web server to serve multiple websites
		\item its a newer protocol and requires the client to indicate the hostname of the target server in the initial handshake
		\item Server will then find the correct cert or return default one
		\item Only works for ALB/NLB and CloudFront
		\item EX: ALB with multiple TGs which point to different hostnames, load the SSL cert for each hostname, using SNI, the LB can point the right SSL cert to the right TG
	\end{itemize}
\end{itemize}

\subsection{Connection Draining}
\begin{itemize}
	\item Time to complete in-flight requests while the instance is de-registering or unhealthy
	\item Stops sending new requests to the instance which is de-registering
	\item default is 300 seconds
\end{itemize}

\section{Classic Load Balancer}
\begin{itemize}
	\item TCP, HTTP, and HTTPS
	\item v1
	\item Fixed hostname
	\item client $\rightarrow$ CLB $\rightarrow$ internal EC2
\end{itemize}

\section{Application Load Balancer}
\begin{itemize}
	\item v2, HTTP
	\item Load balancing to multiple HTTP applications across machines (target groups)
	\item Load balancing to multiple applicants on the same machine (ex: containers)
	\item Routing tables to different target groups
	\begin{itemize}
		\item Routing based on path in URL
		\item Routing based on hostname in URL
		\item Routing based on query string, headers
	\end{itemize}
	\item great fit for micro-services and container based applications
	\item Port mapping feature to redirect to a dynamic port in ECS
	\item EX: ALB hosts multiple microservices which are routed via target groups
	\item Fixed hostname
	\item Application servers don't see the IP of the client directly
	\item The true IP of the client is inserted in the header $\textbf{X-Forwarded-For}$
\end{itemize}

\subsection{Target Groups}
\begin{itemize}
	\item EC2 instances (can be managed by ASG) - HTTP
	\item ECS tasks (managed by ECS itself) - HTTP
	\item Lambda functions - HTTP request translated into a JSON event
	\item IP addresses - private IPs
	\item ALB can route to multiple target groups
\end{itemize}

\section{Network Load Balancer}
\begin{itemize}
	\item v2, TCP/UDP
	\item Less latency, handles millions of requests
	\item Has one static IP per AZ and supports assigning Elastic IP (helpful for whitelisting specific IP)
	\item NLB are used for extreme performance, TCP or UDP traffic
	\item not in free tier
\end{itemize}

\section{Auto Scaling Groups}
\begin{itemize}
	\item in real life, the load on your websites and application can change quickly
	\item the goal is to: scale out (add EC2 instances) to match increased load or scale in by removing EC2 instances
	\item Ensure we have a min and max number of machines running
	\item automatically register new EC2 instances to an LB
	\item ASGs have the following attributes
	\begin{itemize}
		\item AMI + instance type
		\item EC2 user data
		\item EBS volumes
		\item SGs
		\item SSH key pair
		\item Min/Max Size, initial capacity
		\item Network and Subnets information
		\item LB information
		\item Scaling policies
	\end{itemize}
	\item Auto scaling alarms: Can scale ASG based on CW alarms
	\item Auto scaling new rules: "define" better AS rules that are directly managed by the EC2 (ex: CPU usage, avg network in/out, number of requests)
	\item Can create with Launch Template
\end{itemize}

\subsection{ASG Scaling Policies}
\begin{itemize}
	\item Dynamic Scaling Policies
	\begin{itemize}
		\item Target Tracking Scaling
		\begin{itemize}
			\item Easiest to set up
			\item EX: i want the avg ASG CPU to stay around 40 percent
		\end{itemize}
		\item Simple/Step Scaling
		\begin{itemize}
			\item EX: When a CW alarm is triggered (CPU $>$ 70 percent)then add 2 units
			\item EX: when a CW alarm is triggered (CPU $<$ 30 percent) then remove 1 unit
		\end{itemize}
		\item Scheduled Actions
		\begin{itemize}
			\item Anticipate a scaling based on known usage patterns
			\item EX: increase min capacity to 10 at 5 pm friday
		\end{itemize}
	\end{itemize}
	\item Predictive Scaling
	\begin{itemize}
		\item continuously forecast load and schedule scaling ahead
	\end{itemize}
	\item Good metrics to scale on: CPU Utilization, RequestCountPerTarget, Avg Network in/out
	\item Cooldown: there is a cooldown period after a scaling happens. ASG will not launch or terminate additional instances
	\item Use a ready to use AMI to reduce config time in order to be serving requests faster
\end{itemize}

\subsection{ASG for solution architects}
\begin{itemize}
	\item ASG default termination policy
	\begin{enumerate}
		\item Find the AZ which has the most number of instances
		\item If there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration
	\end{enumerate}
	\item ASG tries to balance the number of instances across the AZs by default
	\item Lifecyle Hooks
	\begin{itemize}
		\item By default as soon as an instance is launched it's in service
		\item You have the ability to perform extra steps before the instances goes into service (pending state)
		\item Also ability to perform actions before instance is terminated (terminating state)
	\end{itemize}
	\item Launch Configuration vs Launch Template
	\begin{itemize}
		\item Both: AMI, instance type, SGs, tags
		\item Configuration: legacy, must be re-created every time
		\item Template: newer, can have multiple versions, create parameter subsets, provision using both on-demand and spot instances (recommended by AWS)
	\end{itemize}
	\item Unhealthy EC2 instances will get terminated
\end{itemize}

\end{document}
